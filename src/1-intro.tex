%%%%%

\section{Introduction}
Conceptually, the execution of any computing kernel can be considered as a sequence of interdependent tasks.  Each of these tasks, whether consisting of a single instruction or an entire program, can be viewed as consuming a set of inputs and can act in turn as a producer of the values consumed by other tasks through either mutation of global state or explicit communication of values.  Any particular execution will represent a schedule, serial or parallel, that obeys the dependencies expressed among these producers and consumers.  In the currently dominant models of execution, a key dependency is \emph{data dependency}, expressed by rules that a value (both singletons and multi-dimensional) must be fully computed, i.e., all possible updates must have already been applied (often implemented with termination of the producer task as a temporally conservative proxy) before any consumer of that value can begin execution.

However, under this common formulation of data dependency, there are three potential opportunities for deriving benefits from starting a consumer early, i.e., before traditional data dependency rules would allow its execution to begin. The first opportunity is if the input to the consumer has already attained its final value before all possible updates have been applied. Starting at the earliest point where this is true would be beneficial for any consumer, without restriction, as it would reduce latency without possible sensitivity to its execution. The second opportunity is where, while subsequent updates will change the value, the current value is indistinguishable from the final value from the perspective of the consumer. Any consumer that only consumes an input, e.g., $X$, indirectly as a property $P(X)$ rather than the value of $X$, or as a quantized version of $X$, $Q(X)$ that returns identical values for all values $X$ assumes between the current point in time and when it assumes its final value, may benefit from starting at this point if $P(X)$ or $Q(X)$ have attained their final value. The third opportunity for early execution is that, while subsequent updates to the input $X$ to a consumer task $C$ will change the value of $X$, and do so in ways that change $C(X)$, $C$ is inherently approximate and the difference between $C(X_{current})$ and $C(X_{final})$ is within the approximation tolerance for $C$.

While these opportunities are clear, realizing them requires solving several challenges, not the least of which are the requirement for future knowledge, to ensure the soundness of certain decisions, and a need to understand the application-specific, and potentially dynamic, semantics within graphs of producer-consumer relationships in order to reason about the presence of the second and third opportunities. The scope of all possible uses and implementations of support for eager execution is vast; in this paper, we focus on a specific "data-access-centric" model called {\em Fluid Computation}, and explain the necessary programming language, compiler and runtime support for it. Specifically, this paper makes the following {\bf contributions}:

$\bullet$ We propose {\em Fluid computation}, a specific approach to eager computation based on a novel concept referred to as a {\em Fluid variable}. A Fluid Variable represents a variable on which a consumer function can operate even before it gets its final value (generated by a producer function). In a sense, a Fluid variable is shared between (at least) a producer function and (at least) a consumer function, and its stable controls when the latter can start executing. We also propose (i) specialized functions called {\em Valves} that reflect changes in the states of Fluid variables and (i) specialized expressions, called {\em Guards} that are constructed using valves and control the execution of consumers of Fluid variables. Our current implementation employs two different types of valves, a \textit{counting} valve, allowing the execution of a consumer function only after a certain number of updates have been made to the Fluid variable by the producers, and \textit{stability} valve, allowing the execution of a consumer function only after the Fluid variable has a value that does not change across a certain number of producer updates.

%One of our control conditions, called a {\em Valve}, determines when a consumer function can start its computation based on a condition specified by partially-computed values of a set of Fluid Variables. Our second control condition (Quality-Quantification, or QQ function), on the other hand, determines whether the quality of the output of a consumer function is sufficient to terminate its computation. Finally, the stepping function captures how frequently QQ is to be executed. 

$\bullet$ We detail the programming language, compiler and runtime system support needed to realize Fluid computation. Our language support targets C++ as the base language and extends it with Fluid variables (defined as "Fluid Objects"), valves and guards; our compiler support automatically translates a program written with the proposed language extensions into an equivalent C++ code that can be compiled by any C++compiler; our runtime support involves the management of execution state, which, in the general case, includes keeping track of multiple versions of Fluid variables and coordinating data sharing between producer and consumer methods as well as controlling the start and termination of producer and consumer methods using guard evaluations.

$\bullet$ We report experimental results when applying Fluid computation to four different application programs that are amenable to approximation, namely, K-means clustering, Bellman-Ford SSSP, graph coloring and edge detection. Our experimental analysis indicates that the fluidized versions of  K-means, Bellman-Ford, graph coloring, and edge detection bring, on average, 59\%, 46\%, 16\% and 19\% execution time improvements, respectively, over their original counterparts, under the default values of our fluidization parameters. 



Our experience with Fluid computation indicates that it is highly expressive and can capture a wide variety of eager execution paradigms that exist in the literature with an expectation, as demonstrated for the above four applications, that fluid expressions of eager execution opportunities will realize substantial performance improvements. 

%We use K-means as a running example to demonstrate how Fluid Computation can capture various forms of eager execution, and present experimental data showing its effectiveness in practice. 

% four specific consumer-centric usage scenarios where a programmer could elegantly express opportunities for eager task execution at a function-invocation granularity, namely, 1) speculative invocation, 2) loop-free pipeline parallelism, 3) heuristic-triggered approximation, 4) convergence-managed approximation. We introduce a framework called \emph{Fluid Computation} to provide cross-layer language, compiler, and run-time support for expressing, reasoning about, and exploiting computations over partially computed parameters. We demonstrate the potential of Fluid Computation by showing showing latency improvements \todo{QUANTIFY} for both precise and approximate Fluidized applications executed on real hardware with minimal increases in total processor utilization for precise applications and utilization reductions for approximate applications \todo{QUANTIFY}. \sampson{Would be nice to be able to say something quantitative about working for both serial and parallel applications here too...} Finally, we analyze the limitations of our purely software implementation of Fluid Computation and motivate architectural support features that would better support Fluid Computation and discuss affinities between aspects of Fluid Computation and \todo{less traditional architectural models}.

%The remainder of this paper proceeds as follows. Section~\ref{sec:eagermodel} describes our target use cases in more detail and justifies the design decisions in the Fluid Computation framework in terms of coverage of these targets. Section~\ref{sec:fluid_lang} introduces programming language level support for expressing \textit{fluid} dependencies in terms of type extensions and pragmas. Section~\ref{sec:fluid_compiler} discusses the role of the compiler in supporting Fluid Computation, and Section~\ref{sec:fluid_runtime} covers runtime support. We evaluate our framework in Section~\ref{sec:evaluation} and discuss related work in Section~\ref{sec:related}. Section~\ref{sec:future} motivates directions for future work on Fluid Computation and Section~\ref{sec:conclusion} concludes.




